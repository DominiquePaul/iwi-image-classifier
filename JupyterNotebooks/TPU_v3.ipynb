{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from io import BytesIO \n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "class cnn_model:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initializes the model and defines the graph. There will always be one more\n",
    "        dense layer than defined.\n",
    "        \"\"\"\n",
    "        # hard features\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss = \"binary_crossentropy\"\n",
    "        self.tpu_instance_name = \"dominique-c-a-paul\"\n",
    "\n",
    "    def new_model(self, x_data, y_data, num_classes, config, name=None):\n",
    "        # mutable features\n",
    "        self.conv_layers = int(config[\"conv_layers\"])\n",
    "        self.conv_filters = int(config[\"conv_filters\"])\n",
    "        self.conv_stride = (1,1)\n",
    "        self.kernel_size = (3,3)\n",
    "        self.pool_size = (2,2)\n",
    "        self.pool_stride = (2,2)\n",
    "        self.dense_layers = int(config[\"dense_layers\"])\n",
    "        self.dense_neurons = int(config[\"dense_neurons\"])\n",
    "        self.dropout_rate_dense = config[\"dropout_rate_dense\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.activation_fn = config[\"activation_fn\"]\n",
    "        \n",
    "        # we give the model a name that describes its parameters\n",
    "        if bool(name):\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = \"conv_size_{}_filters_{}_dense_{}_dropout_{}_dense_{}_lr_{}_act_{}\".format(self.conv_layers, self.conv_filters, \n",
    "                                              self.dense_layers, self.dropout_rate_dense, \n",
    "                                              self.learning_rate, self.activation_fn)\n",
    "        \n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = self.load_data(x_data, y_data)\n",
    "        self.model = self.create_model(num_output_classes=num_classes)\n",
    "\n",
    "            \n",
    "    def load_data(self, x_train, y_train):\n",
    "        # check whether input is numpy format or a link to google cloud storage\n",
    "        if isinstance(x_train, str):\n",
    "            if \"gs\" in x_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(x_train, binary_mode=True))\n",
    "                x_train1 = np.load(f)\n",
    "            else:\n",
    "                x_train1 = np.load(x_train)\n",
    "            \n",
    "        if isinstance(y_train, str):\n",
    "            if \"gs\" in y_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(y_train, binary_mode=True))\n",
    "                y_train1 = np.load(f)\n",
    "            else:\n",
    "                y_train1 = np.load(y_train)\n",
    "            \n",
    "        # create train and validation sets\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train1,\n",
    "                                                            y_train1,\n",
    "                                                            train_size=0.8,\n",
    "                                                            random_state = 1) # random state during training, has to be removed later on\n",
    "        return(x_train, x_val, y_train, y_val)\n",
    "        \n",
    "    def create_model(self, num_output_classes):\n",
    "        input_shape = self.x_train.shape[1:]\n",
    "\n",
    "        # defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=self.conv_filters,\n",
    "                         kernel_size=self.kernel_size,\n",
    "                         activation=self.activation_fn,\n",
    "                         input_shape=input_shape,\n",
    "                         padding=\"SAME\",\n",
    "                         strides=self.conv_stride\n",
    "                        ))\n",
    "        model.add(Conv2D(filters=self.conv_filters,\n",
    "                         kernel_size=self.kernel_size,\n",
    "                         activation=self.activation_fn,\n",
    "                         padding=\"SAME\",\n",
    "                         strides=self.conv_stride\n",
    "                        ))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                 strides=self.pool_stride))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        for i in range(self.conv_layers-1):\n",
    "            model.add(Conv2D(filters=self.conv_filters,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             activation=self.activation_fn,\n",
    "                             padding=\"SAME\",\n",
    "                             strides=self.conv_stride\n",
    "                             #input_shape=input_shape\n",
    "                          ))\n",
    "            model.add(Conv2D(filters=self.conv_filters,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             activation=self.activation_fn,\n",
    "                             padding=\"SAME\",\n",
    "                             strides=self.conv_stride\n",
    "                             #input_shape=input_shape\n",
    "                          ))\n",
    "            model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                     strides=self.pool_stride))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        for i in range(self.dense_layers):\n",
    "            model.add(Dense(self.dense_neurons, activation=self.activation_fn))\n",
    "            model.add(Dropout(self.dropout_rate_dense))\n",
    "        model.add(Dense(num_output_classes, activation='softmax')) # softmax remains unchanged\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def train(self, epochs, batch_size, learning_rate=None, optimizer=None, loss=None, verbose=False, on_tpu=False):\n",
    "        \"\"\"\n",
    "        trains the model.\n",
    "\n",
    "        If the initial config file contained parameters for training then\n",
    "        these dont have to be defined but can still be overridden\n",
    "        \"\"\" \n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate \n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer \n",
    "        if loss is None:\n",
    "            loss = self.loss \n",
    "                  \n",
    "        date_time = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "        log_name = \"gs://data-imr-unisg/logs/{}_{}\".format(self.name, date_time)\n",
    "        \n",
    "        # defining callbacks for training\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_name,\n",
    "                                write_graph=True,\n",
    "                                write_images=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=5)\n",
    "        callbacks = [tensorboard_callback, early_stopping_callback]\n",
    "        \n",
    "        # model has to be compiled differently when on tpu\n",
    "        if on_tpu:\n",
    "            self.train_on_tpu(epochs, batch_size, learning_rate, optimizer, loss, callbacks) \n",
    "        else:\n",
    "            self.train_on_cpu(epochs, batch_size, learning_rate, optimizer, loss, callbacks, verbose)\n",
    "            \n",
    "\n",
    "    def train_on_cpu(self, epochs, batch_size, learning_rate, optimizer, loss, callbacks, verbose):\n",
    "        self.y_train = tf.keras.utils.to_categorical(self.y_train, 2 )\n",
    "        self.y_val = tf.keras.utils.to_categorical(self.y_val, 2 )\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', f1_score])\n",
    "        self.model.fit(self.x_train, self.y_train, epochs=epochs, batch_size=batch_size, \n",
    "                       verbose=verbose, callbacks=callbacks, validation_data=(self.x_val, self.y_val))\n",
    "\n",
    "    def train_on_tpu(self, epochs, batch_size, learning_rate, optimizer, loss, callbacks):\n",
    "        self.model = tf.contrib.tpu.keras_to_tpu_model(self.model, strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(self.tpu_instance_name)))\n",
    "        self.model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy', f1_score])\n",
    "\n",
    "        # has to be optimised to really train a epoch with full data\n",
    "        self.hist = self.model.fit_generator(\n",
    "            self.train_gen(batch_size),\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=10, # still have to change this\n",
    "            validation_data=(self.x_val, self.y_val),\n",
    "            callbacks=callbacks\n",
    "            )\n",
    "        \n",
    "        self.model = self.model.sync_to_cpu()\n",
    "\n",
    "    def train_gen(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generator function for train_on_tpu which provides batches of data\n",
    "        generator function for training the model on a tpu\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            offset = np.random.randint(0, self.x_train.shape[0] - batch_size)\n",
    "            # print(self.x_train[offset:offset+batch_size].shape, self.y_train[offset:offset + batch_size].shape)\n",
    "            yield self.x_train[offset:offset+batch_size], self.y_train[offset:offset + batch_size]\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        predictions = self.model.predict(x_data)\n",
    "        return predictions\n",
    "\n",
    "    def predict_classes(self,x_data):\n",
    "        predicted_classes = self.model.predict_classes(x_data)\n",
    "        return(predicted_classes)\n",
    "\n",
    "\n",
    "    def save_model(self, folder_path=\"./\", name=None):\n",
    "        if bool(name) == False:\n",
    "            name = self.name\n",
    "            \n",
    "        file_path = os.path.join(folder_path, name + \".HDF5\")\n",
    "        tf.keras.models.save_model(self.model,\n",
    "                                   file_path,\n",
    "                                   overwrite=True,\n",
    "                                   include_optimizer=False) # we dont need the optimizer as we only finished ready models\n",
    "        print(\"Model: {} was saved\".format(name))\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        self.model = tf.keras.models.load_model(file_path,\n",
    "                                                 compile=False)\n",
    "        print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_v1 = {    \n",
    "    \"conv_layers\": 4,\n",
    "    \"conv_filters\": 128,\n",
    "    \"dense_layers\": 5,\n",
    "    \"dense_neurons\": 20,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "# offline\n",
    "x_train = \"/Users/dominiquepaul/xBachelorArbeit/Daten/3-Spring19/1-OwnNetwork/np_array_files/x_train.npy\"\n",
    "y_train = \"/Users/dominiquepaul/xBachelorArbeit/Daten/3-Spring19/1-OwnNetwork/np_array_files/class_labels_train.npy\"\n",
    "# online\n",
    "x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "x_test_url = \"gs://data-imr-unisg/np_array_files/x_test.npy\"\n",
    "y_test_url = \"gs://data-imr-unisg/np_array_files/class_labels_test.npy\"\n",
    "f1 = BytesIO(file_io.read_file_to_string(x_test_url, binary_mode=True))\n",
    "f2 = BytesIO(file_io.read_file_to_string(y_test_url, binary_mode=True))\n",
    "x_test = np.load(f1)\n",
    "y_test = np.load(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "print(\"New Model\")\n",
    "m1 = cnn_model()\n",
    "m1.new_model(x_train_url, y_train_url, 2, config_v1)\n",
    "print(\"Training model...\")\n",
    "m1.train(epochs=2, batch_size=256,on_tpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Existing Model & Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully, but was not compiled. Only predictions possible, not training\n"
     ]
    }
   ],
   "source": [
    "m2 = cnn_model()\n",
    "m2.load_model(\"meins_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m2.predict(x_test)\n",
    "cls_preds = m2.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Trying to convert predictions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = np.stack([cls_preds[:10], cls_preds[:10], cls_preds[:10]])\n",
    "stacked[1][2] = 3\n",
    "df = pd.DataFrame(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.apply_along_axis(np.bincount, 0, stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
