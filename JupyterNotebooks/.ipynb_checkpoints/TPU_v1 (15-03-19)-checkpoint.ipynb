{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Link (TPU tutorial):\n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=SaYPv_aKId2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "We train our own network on a TPU. The network consists out of stacked conv, pool and dropout layers\n",
    "\n",
    "## Open To Do's:\n",
    "* Add a mathematical part, that checks whether or not the stride size of the pooling layer is too large in combination with the amount of layers and will cause an error\n",
    "* Larger Hyperparameter testing\n",
    "* Replace dropout by Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install keras-metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from io import BytesIO \n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "    def __init__(self, x_train, y_train,num_classes, config, name=None):\n",
    "        \"\"\"\n",
    "        initializes the model and defines the graph. There will always be one more\n",
    "        dense layer than defined.\n",
    "        \"\"\"\n",
    "        # hard features\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss = \"binary_crossentropy\"\n",
    "        \n",
    "        # mutable features\n",
    "        self.conv_layers = int(config[\"conv_layers\"])\n",
    "        self.conv_filters = int(config[\"conv_filters\"])\n",
    "        self.conv_stride = (int(config[\"conv_stride\"]), int(config[\"conv_stride\"]))\n",
    "        self.kernel_size = (int(config[\"kernel_size\"]), int(config[\"kernel_size\"]))\n",
    "        self.pool_size = (int(config[\"pool_size\"]), int(config[\"pool_size\"]))\n",
    "        self.pool_stride = (int(config[\"pool_stride\"]),int(config[\"pool_stride\"]))\n",
    "        self.dense_layers = int(config[\"dense_layers\"])\n",
    "        self.dense_neurons = int(config[\"dense_neurons\"])\n",
    "        self.dropout_rate_dense = config[\"dropout_rate_dense\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.activation_fn = config[\"activation_fn\"]\n",
    "        # name describes characteristics \n",
    "        if bool(name):\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = \"conv_size_{}_filters_{}_kernel_{}_pool_{}_dense_{}_dropout_\\\n",
    "                dense_{}_lr_{}_act_{}\".format(self.conv_layers, self.conv_filters, self.kernel_size[0], \n",
    "                self.pool_size[0], self.dense_layers, self.dropout_rate_dense, \n",
    "                self.learning_rate, self.activation_fn)\n",
    "        \n",
    "        # check whether input is numpy format or a link to google cloud storage\n",
    "        if isinstance(x_train, str):\n",
    "            if \"gs\" in x_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(x_train, binary_mode=True))\n",
    "                self.x_train = np.load(f)\n",
    "        else:\n",
    "            self.x_train = x_train\n",
    "            \n",
    "        if isinstance(y_train, str):\n",
    "            if \"gs\" in y_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(y_train, binary_mode=True))\n",
    "                self.y_train = np.load(f)\n",
    "        else:\n",
    "            self.y_train = y_train\n",
    "            \n",
    "        # create train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train,\n",
    "                                                            self.y_train,\n",
    "                                                            train_size=0.8,\n",
    "                                                            random_state = 1)\n",
    "        input_shape = self.x_train.shape[1:]\n",
    "\n",
    "        # defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=self.conv_filters,\n",
    "                         kernel_size=self.kernel_size,\n",
    "                         activation=self.activation_fn,\n",
    "                         input_shape=input_shape,\n",
    "                         padding=\"SAME\",\n",
    "                         strides=self.conv_stride\n",
    "                        ))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                 strides=self.pool_stride))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        for i in range(self.conv_layers-1):\n",
    "            model.add(Conv2D(filters=self.conv_filters,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             activation=self.activation_fn,\n",
    "                             padding=\"SAME\",\n",
    "                             strides=self.conv_stride\n",
    "                             #input_shape=input_shape\n",
    "                          ))\n",
    "            model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                     strides=self.pool_stride))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        for i in range(self.dense_layers):\n",
    "            model.add(Dense(self.dense_neurons, activation=self.activation_fn))\n",
    "            model.add(Dropout(self.dropout_rate_dense))\n",
    "\n",
    "        model.add(Dense(num_classes, activation='softmax')) # softmax remains unchanged\n",
    "        self.model = model   \n",
    "        \n",
    "    def train(self, epochs, batch_size, learning_rate=None, optimizer=None, loss=None, verbose=False, on_tpu=False):\n",
    "        \"\"\"\n",
    "        trains the model.\n",
    "\n",
    "        If the initial config file contained parameters for training then\n",
    "        these dont have to be defined but can still be overridden\n",
    "        \"\"\" \n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate \n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer \n",
    "        if loss is None:\n",
    "            loss = self.loss \n",
    "                  \n",
    "        date_time = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "        log_name = \"gs://data-imr-unisg/logs/{}_{}\".format(self.name, date_time)\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_name,\n",
    "                                write_graph=True,\n",
    "                                write_images=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=5)\n",
    "        \n",
    "        # model has to be compiled differently when on tpu\n",
    "        if on_tpu:\n",
    "            \n",
    "            tpu_model = tf.contrib.tpu.keras_to_tpu_model(self.model,\n",
    "                                                          strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(\"dominique-c-a-paul\")))\n",
    "            tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy', f1_score])\n",
    "        \n",
    "            def train_gen(batch_size):\n",
    "                \"\"\"\n",
    "                generator function for training the model on a tpu\n",
    "                \"\"\"\n",
    "                while True:\n",
    "                    offset = np.random.randint(0, self.x_train.shape[0] - batch_size)\n",
    "                    # print(self.x_train[offset:offset+batch_size].shape, self.y_train[offset:offset + batch_size].shape)\n",
    "                    yield self.x_train[offset:offset+batch_size], self.y_train[offset:offset + batch_size]\n",
    "\n",
    "            # has to be optimised to really train a epoch with full data\n",
    "            self.hist = tpu_model.fit_generator(\n",
    "                train_gen(batch_size),\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=10,\n",
    "                validation_data=(self.x_val, self.y_val),\n",
    "                callbacks=[tensorboard_callback]\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            self.y_train = tf.keras.utils.to_categorical(self.y_train, 2 )\n",
    "            self.y_val = tf.keras.utils.to_categorical(self.y_val, 2 )\n",
    "            self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', f1])\n",
    "            self.model.fit(self.x_train, self.y_train, epochs=epochs, batch_size=batch_size, \n",
    "                           verbose=verbose, callbacks=[tensorboard_callback], validation_data=(self.x_val, self.y_val))\n",
    "\n",
    "        def predict(self, x_data):\n",
    "            cpu_model = tpu_model.sync_to_cpu()\n",
    "            predictions = cpu_model.predict(x_data)\n",
    "            return predictions\n",
    "\n",
    "        \n",
    "        def save_model(self, file_path):\n",
    "            raise notImplementedError()\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 299, 299, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 149, 149, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 149, 149, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 74, 74, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 74, 74, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 37, 37, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                829460    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,279,566\n",
      "Trainable params: 1,278,542\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/1\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 299, 299, 3), dtype=tf.float32, name='conv2d_16_input_10'), TensorSpec(shape=(16, 1), dtype=tf.float32, name='dense_23_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_16_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 52.128994941711426 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 9/10 [==========================>...] - ETA: 7s - loss: 0.8862 - sparse_categorical_accuracy: 0.5694 - f1_score: 0.8473 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 299, 299, 3), dtype=tf.float32, name='conv2d_16_input_10'), TensorSpec(shape=(16, 1), dtype=tf.float32, name='dense_23_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_16_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 49.965601444244385 secs\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(14,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(14, 299, 299, 3), dtype=tf.float32, name='conv2d_16_input_10'), TensorSpec(shape=(14, 1), dtype=tf.float32, name='dense_23_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_16_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 36.88059711456299 secs\n",
      "10/10 [==============================] - 161s 16s/step - loss: 0.8704 - sparse_categorical_accuracy: 0.5898 - f1_score: 0.8518 - val_loss: 8.7728 - val_sparse_categorical_accuracy: 0.2750 - val_f1_score: 0.8406\n"
     ]
    }
   ],
   "source": [
    "config_v1 = {    \n",
    "    \"conv_layers\": 4,\n",
    "    \"conv_filters\": 128,\n",
    "    \"conv_stride\": 1,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":2,\n",
    "    \"pool_stride\": 2,\n",
    "    \"dense_layers\": 5,\n",
    "    \"dense_neurons\": 20,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m1 = cnn_model(x_train_url, y_train_url, 2, config_v1)\n",
    "m1.model.summary()\n",
    "m1.train(on_tpu=True, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_opt=cnn_model(x_train_url, y_train_url, 2, config_v2)\n",
    "hist = m_opt.train(on_tpu=True, epochs=2, batch_size=128)\n",
    "val_loss = np.max(m_opt.hist.history[\"val_loss\"])\n",
    "val_accuracy = np.max(m_opt.hist.history[\"val_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_opt.hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !gcloud alpha compute tpus list\n",
    "# !gcloud compute tpus stop dominique-c-a-paul && gcloud compute tpus start dominique-c-a-paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: \n",
    "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n",
    "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 299, 299, 82)      328       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 149, 149, 82)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 149, 149, 82)      328       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 149, 149, 82)      6806      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 74, 74, 82)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 74, 74, 82)        328       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 74, 74, 82)        6806      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 36, 36, 82)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 36, 36, 82)        328       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 36, 36, 82)        6806      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 17, 17, 82)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 17, 17, 82)        328       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 17, 17, 82)        6806      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 8, 8, 82)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 8, 8, 82)          328       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5248)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 74)                388426    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 74)                5550      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 150       \n",
      "=================================================================\n",
      "Total params: 423,318                              \n",
      "Trainable params: 422,498                          \n",
      "Non-trainable params: 820                          \n",
      "_________________________________________________________________\n",
      "None                                               \n",
      "  0%|          | 0/2 [00:03<?, ?it/s, best loss: ?]INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/2                                          \n",
      "  0%|          | 0/2 [00:19<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(32, 299, 299, 3), dtype=tf.float32, name='conv2d_20_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_26_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_20_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 46.229596853256226 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 8:10 - loss: 3.0649 - sparse_categorical_accuracy: 0.5781 - f1_score: 0.7792\n",
      "                                                  \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 3:45 - loss: 3.2919 - sparse_categorical_accuracy: 0.5684 - f1_score: 0.8232\n",
      "                                                  \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 2:15 - loss: 3.3558 - sparse_categorical_accuracy: 0.5807 - f1_score: 0.8396\n",
      "                                                  \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 1:29 - loss: 3.4386 - sparse_categorical_accuracy: 0.5674 - f1_score: 0.8440\n",
      "                                                  \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 1:01 - loss: 3.2756 - sparse_categorical_accuracy: 0.5766 - f1_score: 0.8440\n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 42s - loss: 3.2259 - sparse_categorical_accuracy: 0.5833 - f1_score: 0.8449 \n",
      "                                                   \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 28s - loss: 3.1481 - sparse_categorical_accuracy: 0.5876 - f1_score: 0.8459\n",
      "                                                   \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 16s - loss: 3.0739 - sparse_categorical_accuracy: 0.5923 - f1_score: 0.8467\n",
      "                                                   \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 7s - loss: 3.0294 - sparse_categorical_accuracy: 0.6003 - f1_score: 0.8487 \n",
      "  0%|          | 0/2 [01:29<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(30,), dtype=tf.int32, name='core_id_30'), TensorSpec(shape=(30, 299, 299, 3), dtype=tf.float32, name='conv2d_20_input_10'), TensorSpec(shape=(30, 1), dtype=tf.float32, name='dense_26_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_20_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 53.65121531486511 secs\n",
      "                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================]             \n",
      " - 128s 13s/step - loss: 3.0460 - sparse_categorical_accuracy: 0.6027 - f1_score: 0.8472 - val_loss: 1.4561 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "Epoch 2/2                                          \n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 17s - loss: 2.6219 - sparse_categorical_accuracy: 0.6484 - f1_score: 0.8468\n",
      "                                                   \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 14s - loss: 2.8254 - sparse_categorical_accuracy: 0.6328 - f1_score: 0.8455\n",
      "                                                   \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 13s - loss: 2.8270 - sparse_categorical_accuracy: 0.6406 - f1_score: 0.8544\n",
      "                                                   \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 11s - loss: 2.8753 - sparse_categorical_accuracy: 0.6328 - f1_score: 0.8499\n",
      "                                                   \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 9s - loss: 2.9064 - sparse_categorical_accuracy: 0.6312 - f1_score: 0.8534 \n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 7s - loss: 2.8555 - sparse_categorical_accuracy: 0.6367 - f1_score: 0.8553\n",
      "                                                  \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 5s - loss: 2.8183 - sparse_categorical_accuracy: 0.6356 - f1_score: 0.8552\n",
      "                                                  \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 3s - loss: 2.7385 - sparse_categorical_accuracy: 0.6416 - f1_score: 0.8545\n",
      "                                                  \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 1s - loss: 2.6788 - sparse_categorical_accuracy: 0.6506 - f1_score: 0.8553\n",
      "                                                  \n",
      "10/10 [==============================]             \n",
      " - 23s 2s/step - loss: 2.6279 - sparse_categorical_accuracy: 0.6555 - f1_score: 0.8553 - val_loss: 1.5382 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "  0%|          | 0/2 [02:50<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "InvalidLoss",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m                     \u001b[0mdict_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidLoss\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1ce307be36a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m best = fmin(fn = objective, space = space, algo = tpe.suggest, \n\u001b[0;32m---> 68\u001b[0;31m             max_evals = MAX_EVALS, trials = bayes_trials)\n\u001b[0m",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0mdict_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattach_attachments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidLoss\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "MAX_EVALS = 2\n",
    "\n",
    "\n",
    "# File to save first results\n",
    "out_file = 'bayes_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "# Write the headers to the file\n",
    "writer.writerow(['params','run_time', 'val_loss', 'val_accuracy', 'train_loss', 'train_accuracy', 'train_f1'])\n",
    "of_connection.close()\n",
    "\n",
    "# hyperparameter optimization with hyperopt\n",
    "def objective(params):\n",
    "    m_opt=cnn_model(x_train_url, y_train_url, 2, params)\n",
    "    print(m_opt.model.summary())\n",
    "    start = timer()\n",
    "    m_opt.train(on_tpu=True, epochs=2, batch_size=256)\n",
    "    run_time = timer() - start\n",
    "    val_loss = m_opt.hist.history[\"val_loss\"][-1]\n",
    "    val_accuracy = m_opt.hist.history[\"val_sparse_categorical_accuracy\"][-1]\n",
    "    val_f1 = m_opt.hist.history[\"val_f1_score\"][-1]\n",
    "    train_loss = m_opt.hist.history[\"loss\"][-1]\n",
    "    train_accuracy = m_opt.hist.history[\"sparse_categorical_accuracy\"][-1]\n",
    "    train_f1 = m_opt.hist.history[\"f1_score\"][-1]\n",
    "    \n",
    "    # adding lines to csv\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([params, run_time, val_loss, val_accuracy, train_loss, train_accuracy, train_f1])\n",
    "    of_connection.close()\n",
    "    \n",
    "    return {\"loss\": val_loss,\n",
    "            \"params\": params, \n",
    "            \"status\": hyperopt.STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    \"conv_layers\": hp.quniform(\"conv_layers\", 4, 8, 1),\n",
    "    \"conv_filters\": hp.quniform(\"conv_filters\", 2, 128, 1),\n",
    "    \"conv_stride\": hp.quniform(\"conv_stride\", 1, 1, 1),\n",
    "    \"kernel_size\": hp.quniform(\"kernel_size\",1, 2, 1),\n",
    "    \"pool_size\": hp.quniform(\"pool_size\",1, 3, 1),\n",
    "    \"pool_stride\": hp.quniform(\"pool_stride\", 2, 2, 1),\n",
    "    \"dense_layers\": hp.quniform(\"dense_layers\", 1, 5, 1),\n",
    "    \"dense_neurons\": hp.quniform(\"dense_neurons\", 1, 100, 1),\n",
    "    \"dropout_rate_dense\": hp.uniform(\"dropout_rate_dense\",0,1),\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', np.log(1e-02), np.log(1e-06)),\n",
    "    \"activation_fn\": hp.choice('activation_fn', [\"relu\"])\n",
    "}\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "* Write best to disk\n",
    "* Write individual steps to disk as csv\n",
    "* Implement a \"predict\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "NUM_MODELS = 10\n",
    "\n",
    "# build models\n",
    "# config_file_ensemble = placeholder\n",
    "models = []\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    models[i] = cnn_model(x_train_url, y_train_url, 2, config_v2, name=\"ensemble_model_v1_{}\".format())\n",
    "    models[i].train(on_tpu=True, epochs= 10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_v1 = {\n",
    "    \"name\": \"vanilla_v1\",\n",
    "    \"num_classes\": 10,\n",
    "    \"conv_neurons\": [32,64,32,16],\n",
    "    \"conv_dropout_layers\": [0.]*4,\n",
    "    \"dense_neurons\": [100,10],\n",
    "    \"dense_dropout_layers\": [0.]*2,\n",
    "    \"kernel_size\": (3,3),\n",
    "    \"pool_size\": (2,2),\n",
    "    \"activation_fn\": \"relu\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e04,\n",
    "    \"optimizer\": \"adam\", #tf.keras.optimizers.Adam,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Numpy data from GCS\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_train.npy', binary_mode=True))\n",
    "x_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_trainp.npy', binary_mode=True))\n",
    "y_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_test.npy', binary_mode=True))\n",
    "x_test_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_test.npy', binary_mode=True))\n",
    "y_test_gs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train_mnist = np.reshape(x_train_mnist, (-1,28,28,1))\n",
    "x_test_mnist = np.reshape(x_test_mnist, (-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time  # for mnist training \n",
    "\n",
    "config_v2 = {    \n",
    "    \"conv_layers\": 1,\n",
    "    \"conv_filters\": 16,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":4,\n",
    "    \"dense_layers\": 1,\n",
    "    \"dense_neurons\": 10,\n",
    "    \"dropout_rate_conv\": 0.2,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "# x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "# y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m2 = cnn_model(x_train_mnist, y_train_mnist, 10,config_v2)\n",
    "m2.train(on_tpu=True, epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading and reshaping MNIST data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['no_car', 'car']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to save files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def get_files(mypath):\n",
    "    \"\"\"\n",
    "    list all files in a folder\n",
    "    \n",
    "    args: \n",
    "        mypath: directory (str)\n",
    "    returns:\n",
    "        f: list of all file paths relative to directory specified\n",
    "    \"\"\"\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for file in filenames:\n",
    "            f.extend([os.path.join(dirpath,file)])\n",
    "    return f\n",
    "\n",
    "\n",
    "def copy_folder_files_to_gs(folder, gs_folder):\n",
    "    \"\"\"\n",
    "    Copy model.h5 over to Google Cloud Storage\n",
    "    \"\"\"\n",
    "    for file in get_files(folder):\n",
    "        #file_path = os.path.join(folder, \"/\".join(file.split(\"/\")[:]))\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "\n",
    "        with file_io.FileIO(file, mode='r') as input_f:\n",
    "            with file_io.FileIO(gs_folder, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read())\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger1 = \"./logs/1552722826.7797823/events.out.tfevents.1552722838.hsg-iwi-imr2\"\n",
    "logger11=\"TPU_first_test (15-03-19).ipynb\"\n",
    "bucket = \"gs://data-imr-unisg/a\"\n",
    "logger2 = \"logs\"\n",
    "with file_io.FileIO(logger1, mode='r') as input_f:\n",
    "            with file_io.FileIO(bucket, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read()\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
