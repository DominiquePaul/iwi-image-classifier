{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Link (TPU tutorial):\n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=SaYPv_aKId2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "We train our own network on a TPU. The network consists out of stacked conv, pool and dropout layers\n",
    "\n",
    "## Open To Do's:\n",
    "* Changed:\n",
    "    * Two conv layers before pool\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install keras-metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from io import BytesIO \n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "    def __init__(self, x_train, y_train, num_classes, config, name=None):\n",
    "        \"\"\"\n",
    "        initializes the model and defines the graph. There will always be one more\n",
    "        dense layer than defined.\n",
    "        \"\"\"\n",
    "        # hard features\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss = \"binary_crossentropy\"\n",
    "        \n",
    "        # mutable features\n",
    "        self.conv_layers = int(config[\"conv_layers\"])\n",
    "        self.conv_filters = int(config[\"conv_filters\"])\n",
    "        self.conv_stride = (int(config[\"conv_stride\"]), int(config[\"conv_stride\"]))\n",
    "        self.kernel_size = (int(config[\"kernel_size\"]), int(config[\"kernel_size\"]))\n",
    "        self.pool_size = (int(config[\"pool_size\"]), int(config[\"pool_size\"]))\n",
    "        self.pool_stride = (int(config[\"pool_stride\"]),int(config[\"pool_stride\"]))\n",
    "        self.dense_layers = int(config[\"dense_layers\"])\n",
    "        self.dense_neurons = int(config[\"dense_neurons\"])\n",
    "        self.dropout_rate_dense = config[\"dropout_rate_dense\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.activation_fn = config[\"activation_fn\"]\n",
    "        # name describes characteristics \n",
    "        if bool(name):\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = \"conv_size_{}_filters_{}_kernel_{}_pool_{}_dense_{}_dropout_\\\n",
    "                dense_{}_lr_{}_act_{}\".format(self.conv_layers, self.conv_filters, self.kernel_size[0], \n",
    "                self.pool_size[0], self.dense_layers, self.dropout_rate_dense, \n",
    "                self.learning_rate, self.activation_fn)\n",
    "        \n",
    "        # check whether input is numpy format or a link to google cloud storage\n",
    "        if isinstance(x_train, str):\n",
    "            if \"gs\" in x_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(x_train, binary_mode=True))\n",
    "                self.x_train = np.load(f)\n",
    "        else:\n",
    "            self.x_train = x_train\n",
    "            \n",
    "        if isinstance(y_train, str):\n",
    "            if \"gs\" in y_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(y_train, binary_mode=True))\n",
    "                self.y_train = np.load(f)\n",
    "        else:\n",
    "            self.y_train = y_train\n",
    "            \n",
    "        # create train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train,\n",
    "                                                            self.y_train,\n",
    "                                                            train_size=0.8,\n",
    "                                                            random_state = 1)\n",
    "        input_shape = self.x_train.shape[1:]\n",
    "\n",
    "        # defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=self.conv_filters,\n",
    "                         kernel_size=self.kernel_size,\n",
    "                         activation=self.activation_fn,\n",
    "                         input_shape=input_shape,\n",
    "                         padding=\"SAME\",\n",
    "                         strides=self.conv_stride\n",
    "                        ))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                 strides=self.pool_stride))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        for i in range(self.conv_layers-1):\n",
    "            model.add(Conv2D(filters=self.conv_filters,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             activation=self.activation_fn,\n",
    "                             padding=\"SAME\",\n",
    "                             strides=self.conv_stride\n",
    "                             #input_shape=input_shape\n",
    "                          ))\n",
    "            model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                     strides=self.pool_stride))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        for i in range(self.dense_layers):\n",
    "            model.add(Dense(self.dense_neurons, activation=self.activation_fn))\n",
    "            model.add(Dropout(self.dropout_rate_dense))\n",
    "\n",
    "        model.add(Dense(num_classes, activation='softmax')) # softmax remains unchanged\n",
    "        self.model = model   \n",
    "        \n",
    "    def train(self, epochs, batch_size, learning_rate=None, optimizer=None, loss=None, verbose=False, on_tpu=False):\n",
    "        \"\"\"\n",
    "        trains the model.\n",
    "\n",
    "        If the initial config file contained parameters for training then\n",
    "        these dont have to be defined but can still be overridden\n",
    "        \"\"\" \n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate \n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer \n",
    "        if loss is None:\n",
    "            loss = self.loss \n",
    "                  \n",
    "        date_time = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "        log_name = \"gs://data-imr-unisg/logs/{}_{}\".format(self.name, date_time)\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_name,\n",
    "                                write_graph=True,\n",
    "                                write_images=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=5)\n",
    "        \n",
    "        # model has to be compiled differently when on tpu\n",
    "        if on_tpu:\n",
    "            \n",
    "            tpu_model = tf.contrib.tpu.keras_to_tpu_model(self.model,\n",
    "                                                          strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(\"dominique-c-a-paul\")))\n",
    "            tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy', f1_score])\n",
    "        \n",
    "            def train_gen(batch_size):\n",
    "                \"\"\"\n",
    "                generator function for training the model on a tpu\n",
    "                \"\"\"\n",
    "                while True:\n",
    "                    offset = np.random.randint(0, self.x_train.shape[0] - batch_size)\n",
    "                    # print(self.x_train[offset:offset+batch_size].shape, self.y_train[offset:offset + batch_size].shape)\n",
    "                    yield self.x_train[offset:offset+batch_size], self.y_train[offset:offset + batch_size]\n",
    "\n",
    "            # has to be optimised to really train a epoch with full data\n",
    "            self.hist = tpu_model.fit_generator(\n",
    "                train_gen(batch_size),\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=10,\n",
    "                validation_data=(self.x_val, self.y_val),\n",
    "                callbacks=[tensorboard_callback]\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            self.y_train = tf.keras.utils.to_categorical(self.y_train, 2 )\n",
    "            self.y_val = tf.keras.utils.to_categorical(self.y_val, 2 )\n",
    "            self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', f1_score])\n",
    "            self.model.fit(self.x_train, self.y_train, epochs=epochs, batch_size=batch_size, \n",
    "                           verbose=verbose, callbacks=[tensorboard_callback], validation_data=(self.x_val, self.y_val))\n",
    "\n",
    "        def predict(self, x_data):\n",
    "            cpu_model = tpu_model.sync_to_cpu()\n",
    "            predictions = cpu_model.predict(x_data)\n",
    "            return predictions\n",
    "\n",
    "        \n",
    "        def save_model(self, folder_path=\"./\", name=self.name):\n",
    "            file_path = os.path.join(folder_path, name + \".HDF5\")\n",
    "            tf.keras.models.save_model(self.model,\n",
    "                                       self.name,\n",
    "                                       overwrite=True,\n",
    "                                       include_optimizer=True\n",
    "                                      )\n",
    "            \n",
    "        def load_model(self, file_path):\n",
    "            self. model = tf.keras.models.load_model(filepath,\n",
    "                                                     compile=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble_cnn():\n",
    "    def __init__(self, num_models, x_train, y_train, num_classes, config):\n",
    "        self.num_models\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            self.models[i] = cnn_model(x_train, y_train, num_classes, config, name=\"ensemble_model_v1_{}\".format(i))\n",
    "            \n",
    "    def train_models(self.epochs, batch_size):\n",
    "        for i in range(self.num_models):\n",
    "            self.models[i].train(on_tpu=True, epochs= epochs, batch_size=batch_size)\n",
    "            \n",
    "    def predict():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 299, 299, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 149, 149, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 149, 149, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 74, 74, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 74, 74, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 37, 37, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                829460    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,279,566\n",
      "Trainable params: 1,278,542\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config_v1 = {    \n",
    "    \"conv_layers\": 4,\n",
    "    \"conv_filters\": 128,\n",
    "    \"conv_stride\": 1,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":2,\n",
    "    \"pool_stride\": 2,\n",
    "    \"dense_layers\": 5,\n",
    "    \"dense_neurons\": 20,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m1 = cnn_model(x_train_url, y_train_url, 2, config_v1)\n",
    "m1.model.summary()\n",
    "#m1.train(on_tpu=False, epochs=1, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'cnn_model' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-09e4bb449c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"my_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'cnn_model' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "m1.save_model(name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'activation_fn',\n",
       " 'conv_filters',\n",
       " 'conv_layers',\n",
       " 'conv_stride',\n",
       " 'dense_layers',\n",
       " 'dense_neurons',\n",
       " 'dropout_rate_dense',\n",
       " 'kernel_size',\n",
       " 'learning_rate',\n",
       " 'loss',\n",
       " 'model',\n",
       " 'name',\n",
       " 'optimizer',\n",
       " 'pool_size',\n",
       " 'pool_stride',\n",
       " 'train',\n",
       " 'x_train',\n",
       " 'x_val',\n",
       " 'y_train',\n",
       " 'y_val']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv_size_4_filters_128_kernel_3_pool_2_dense_5_dropout_                dense_0.2_lr_0.0001_act_relu'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !gcloud alpha compute tpus list\n",
    "# !gcloud compute tpus stop dominique-c-a-paul && gcloud compute tpus start dominique-c-a-paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: \n",
    "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n",
    "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 299, 299, 23)      299       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 149, 149, 23)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 149, 149, 23)      92        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 149, 149, 23)      2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 23)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 74, 74, 23)        92        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 74, 74, 23)        2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 37, 37, 23)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 37, 37, 23)        92        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 37, 37, 23)        2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 18, 18, 23)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 18, 18, 23)        92        \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 18, 18, 23)        2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 9, 9, 23)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 9, 9, 23)          92        \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 23)          2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 23)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 23)          92        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 23)          2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 2, 2, 23)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 23)          92        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 23)          2139      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 23)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 1, 23)          92        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 16,950                               \n",
      "Trainable params: 16,582                           \n",
      "Non-trainable params: 368                          \n",
      "_________________________________________________________________\n",
      "None                                               \n",
      "  0%|          | 0/2 [00:03<?, ?it/s, best loss: ?]INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/2                                          \n",
      "  0%|          | 0/2 [00:21<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(32, 299, 299, 3), dtype=tf.float32, name='conv2d_4_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_8_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 46.264347314834595 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 8:26 - loss: 0.8874 - sparse_categorical_accuracy: 0.6328 - f1_score: 0.8394\n",
      "                                                  \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 3:52 - loss: 0.8605 - sparse_categorical_accuracy: 0.6484 - f1_score: 0.8470\n",
      "                                                  \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 2:20 - loss: 0.8023 - sparse_categorical_accuracy: 0.6628 - f1_score: 0.8563\n",
      "                                                  \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 1:32 - loss: 0.7845 - sparse_categorical_accuracy: 0.6602 - f1_score: 0.8565\n",
      "                                                  \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 1:03 - loss: 0.7809 - sparse_categorical_accuracy: 0.6562 - f1_score: 0.8551\n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 43s - loss: 0.7744 - sparse_categorical_accuracy: 0.6562 - f1_score: 0.8546 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 28s - loss: 0.7619 - sparse_categorical_accuracy: 0.6629 - f1_score: 0.8564\n",
      "                                                   \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 17s - loss: 0.7622 - sparse_categorical_accuracy: 0.6533 - f1_score: 0.8568\n",
      "                                                   \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 7s - loss: 0.7616 - sparse_categorical_accuracy: 0.6523 - f1_score: 0.8560 \n",
      "  0%|          | 0/2 [01:33<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(30,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(30, 299, 299, 3), dtype=tf.float32, name='conv2d_4_input_10'), TensorSpec(shape=(30, 1), dtype=tf.float32, name='dense_8_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_4_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 53.95645356178284 secs\n",
      "                                                  \n",
      "10/10 [==============================]             \n",
      " - 131s 13s/step - loss: 0.7619 - sparse_categorical_accuracy: 0.6516 - f1_score: 0.8571 - val_loss: 0.6265 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "Epoch 2/2                                          \n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 17s - loss: 0.7233 - sparse_categorical_accuracy: 0.6992 - f1_score: 0.8571\n",
      "                                                   \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 15s - loss: 0.7346 - sparse_categorical_accuracy: 0.6699 - f1_score: 0.8481\n",
      "                                                   \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 13s - loss: 0.7166 - sparse_categorical_accuracy: 0.6745 - f1_score: 0.8545\n",
      "                                                   \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 11s - loss: 0.7040 - sparse_categorical_accuracy: 0.6816 - f1_score: 0.8558\n",
      "                                                   \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 9s - loss: 0.7163 - sparse_categorical_accuracy: 0.6836 - f1_score: 0.8586 \n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 7s - loss: 0.7181 - sparse_categorical_accuracy: 0.6849 - f1_score: 0.8570\n",
      "                                                  \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 5s - loss: 0.7109 - sparse_categorical_accuracy: 0.6853 - f1_score: 0.8596\n",
      "                                                  \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 3s - loss: 0.7164 - sparse_categorical_accuracy: 0.6836 - f1_score: 0.8583\n",
      "                                                  \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 1s - loss: 0.7201 - sparse_categorical_accuracy: 0.6845 - f1_score: 0.8582\n",
      "                                                  \n",
      "10/10 [==============================]             \n",
      " - 20s 2s/step - loss: 0.7196 - sparse_categorical_accuracy: 0.6793 - f1_score: 0.8573 - val_loss: 0.6504 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "0.6504191160202026                                 \n",
      "_________________________________________________________________            \n",
      "Layer (type)                 Output Shape              Param #               \n",
      "=================================================================            \n",
      "conv2d_12 (Conv2D)           (None, 299, 299, 73)      292                   \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_12 (MaxPooling (None, 149, 149, 73)      0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_12 (Batc (None, 149, 149, 73)      292                   \n",
      "_________________________________________________________________            \n",
      "conv2d_13 (Conv2D)           (None, 149, 149, 73)      5402                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_13 (MaxPooling (None, 74, 74, 73)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_13 (Batc (None, 74, 74, 73)        292                   \n",
      "_________________________________________________________________            \n",
      "conv2d_14 (Conv2D)           (None, 74, 74, 73)        5402                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_14 (MaxPooling (None, 37, 37, 73)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_14 (Batc (None, 37, 37, 73)        292                   \n",
      "_________________________________________________________________            \n",
      "conv2d_15 (Conv2D)           (None, 37, 37, 73)        5402                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_15 (MaxPooling (None, 18, 18, 73)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_15 (Batc (None, 18, 18, 73)        292                   \n",
      "_________________________________________________________________            \n",
      "conv2d_16 (Conv2D)           (None, 18, 18, 73)        5402                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_16 (MaxPooling (None, 9, 9, 73)          0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_16 (Batc (None, 9, 9, 73)          292                   \n",
      "_________________________________________________________________            \n",
      "conv2d_17 (Conv2D)           (None, 9, 9, 73)          5402                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 73)          0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_17 (Batc (None, 4, 4, 73)          292                   \n",
      "_________________________________________________________________            \n",
      "flatten_2 (Flatten)          (None, 1168)              0                     \n",
      "_________________________________________________________________            \n",
      "dense_9 (Dense)              (None, 9)                 10521                 \n",
      "_________________________________________________________________            \n",
      "dropout_7 (Dropout)          (None, 9)                 0                     \n",
      "_________________________________________________________________            \n",
      "dense_10 (Dense)             (None, 9)                 90                    \n",
      "_________________________________________________________________            \n",
      "dropout_8 (Dropout)          (None, 9)                 0                     \n",
      "_________________________________________________________________            \n",
      "dense_11 (Dense)             (None, 9)                 90                    \n",
      "_________________________________________________________________            \n",
      "dropout_9 (Dropout)          (None, 9)                 0                     \n",
      "_________________________________________________________________            \n",
      "dense_12 (Dense)             (None, 2)                 20                    \n",
      "=================================================================            \n",
      "Total params: 39,775                                                         \n",
      "Trainable params: 38,899                                                     \n",
      "Non-trainable params: 876                                                    \n",
      "_________________________________________________________________            \n",
      "None                                                                         \n",
      " 50%|█████     | 1/2 [02:57<02:53, 173.24s/it, best loss: 0.6504191160202026]INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/2                                                                    \n",
      " 50%|█████     | 1/2 [03:13<02:53, 173.24s/it, best loss: 0.6504191160202026]INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_20'), TensorSpec(shape=(32, 299, 299, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_12_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 45.01463460922241 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 1/10 [==>...........................]                                       \n",
      " - ETA: 14:13 - loss: 1.1078 - sparse_categorical_accuracy: 0.6875 - f1_score: 0.3780\n",
      "                                                                             \n",
      " 2/10 [=====>........................]                                       \n",
      " - ETA: 6:26 - loss: 1.2749 - sparse_categorical_accuracy: 0.6738 - f1_score: 0.6163 \n",
      "                                                                            \n",
      " 3/10 [========>.....................]                                       \n",
      " - ETA: 3:49 - loss: 1.2389 - sparse_categorical_accuracy: 0.6979 - f1_score: 0.7024\n",
      "                                                                            \n",
      " 4/10 [===========>..................]                                       \n",
      " - ETA: 2:30 - loss: 1.1398 - sparse_categorical_accuracy: 0.6973 - f1_score: 0.7379\n",
      "                                                                            \n",
      " 5/10 [==============>...............]                                       \n",
      " - ETA: 1:42 - loss: 1.0960 - sparse_categorical_accuracy: 0.6992 - f1_score: 0.7607\n",
      "                                                                            \n",
      " 6/10 [=================>............]                                       \n",
      " - ETA: 1:09 - loss: 1.0471 - sparse_categorical_accuracy: 0.7031 - f1_score: 0.7776\n",
      "                                                                            \n",
      " 7/10 [====================>.........]                                       \n",
      " - ETA: 45s - loss: 1.0574 - sparse_categorical_accuracy: 0.6953 - f1_score: 0.7883 \n",
      "                                                                             \n",
      " 8/10 [=======================>......]                                       \n",
      " - ETA: 26s - loss: 1.0518 - sparse_categorical_accuracy: 0.7002 - f1_score: 0.7981\n",
      "                                                                             \n",
      " 9/10 [==========================>...]                                       \n",
      " - ETA: 12s - loss: 1.0302 - sparse_categorical_accuracy: 0.7010 - f1_score: 0.8033\n",
      " 50%|█████     | 1/2 [05:03<02:53, 173.24s/it, best loss: 0.6504191160202026]INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(30,), dtype=tf.int32, name='core_id_30'), TensorSpec(shape=(30, 299, 299, 3), dtype=tf.float32, name='conv2d_12_input_10'), TensorSpec(shape=(30, 1), dtype=tf.float32, name='dense_12_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_12_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 54.956154346466064 secs\n",
      "                                                                             \n",
      "10/10 [==============================]                                       \n",
      " - 170s 17s/step - loss: 1.0330 - sparse_categorical_accuracy: 0.7031 - f1_score: 0.8081 - val_loss: 0.6197 - val_sparse_categorical_accuracy: 0.7292 - val_f1_score: 0.8406\n",
      "\n",
      "Epoch 2/2                                                                    \n",
      " 1/10 [==>...........................]                                       \n",
      " - ETA: 17s - loss: 0.9927 - sparse_categorical_accuracy: 0.7188 - f1_score: 0.8571\n",
      "                                                                             \n",
      " 2/10 [=====>........................]                                       \n",
      " - ETA: 15s - loss: 0.8881 - sparse_categorical_accuracy: 0.7207 - f1_score: 0.8634\n",
      "                                                                             \n",
      " 3/10 [========>.....................]                                       \n",
      " - ETA: 13s - loss: 0.8495 - sparse_categorical_accuracy: 0.7305 - f1_score: 0.8647\n",
      "                                                                             \n",
      " 4/10 [===========>..................]                                       \n",
      " - ETA: 11s - loss: 0.8274 - sparse_categorical_accuracy: 0.7363 - f1_score: 0.8666\n",
      "                                                                             \n",
      " 5/10 [==============>...............]                                       \n",
      " - ETA: 9s - loss: 0.8363 - sparse_categorical_accuracy: 0.7273 - f1_score: 0.8606 \n",
      "                                                                            \n",
      " 6/10 [=================>............]                                       \n",
      " - ETA: 7s - loss: 0.8508 - sparse_categorical_accuracy: 0.7227 - f1_score: 0.8574\n",
      "                                                                            \n",
      " 7/10 [====================>.........]                                       \n",
      " - ETA: 5s - loss: 0.8445 - sparse_categorical_accuracy: 0.7199 - f1_score: 0.8544\n",
      "                                                                            \n",
      " 8/10 [=======================>......]                                       \n",
      " - ETA: 3s - loss: 0.8250 - sparse_categorical_accuracy: 0.7231 - f1_score: 0.8557\n",
      "                                                                            \n",
      " 9/10 [==========================>...]                                       \n",
      " - ETA: 1s - loss: 0.8088 - sparse_categorical_accuracy: 0.7253 - f1_score: 0.8556\n",
      "                                                                            \n",
      "10/10 [==============================]                                       \n",
      " - 21s 2s/step - loss: 0.8013 - sparse_categorical_accuracy: 0.7250 - f1_score: 0.8552 - val_loss: 0.6374 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "0.6374040246009827                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [06:25<00:00, 184.92s/it, best loss: 0.6374040246009827]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "MAX_EVALS = 2\n",
    "\n",
    "\n",
    "# File to save first results\n",
    "out_file = 'bayes_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "# Write the headers to the file\n",
    "writer.writerow(['params','run_time', 'val_loss', 'val_accuracy', 'train_loss', 'train_accuracy', 'train_f1'])\n",
    "of_connection.close()\n",
    "\n",
    "# hyperparameter optimization with hyperopt\n",
    "def objective(params):\n",
    "    m_opt=cnn_model(x_train_url, y_train_url, 2, params)\n",
    "    print(m_opt.model.summary())\n",
    "    start = timer()\n",
    "    m_opt.train(on_tpu=True, epochs=2, batch_size=256)\n",
    "    run_time = timer() - start\n",
    "    val_loss = m_opt.hist.history[\"val_loss\"][-1]\n",
    "    val_accuracy = m_opt.hist.history[\"val_sparse_categorical_accuracy\"][-1]\n",
    "    val_f1 = m_opt.hist.history[\"val_f1_score\"][-1]\n",
    "    train_loss = m_opt.hist.history[\"loss\"][-1]\n",
    "    train_accuracy = m_opt.hist.history[\"sparse_categorical_accuracy\"][-1]\n",
    "    train_f1 = m_opt.hist.history[\"f1_score\"][-1]\n",
    "    \n",
    "    # adding lines to csv\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([params, run_time, val_loss, val_accuracy, train_loss, train_accuracy, train_f1])\n",
    "    of_connection.close()\n",
    "    \n",
    "    print(val_loss)\n",
    "    \n",
    "    return {\"loss\": val_loss,\n",
    "            \"params\": params, \n",
    "            \"status\": hyperopt.STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    \"conv_layers\": hp.quniform(\"conv_layers\", 4, 8, 1),\n",
    "    \"conv_filters\": hp.quniform(\"conv_filters\", 2, 128, 1),\n",
    "    \"conv_stride\": hp.quniform(\"conv_stride\", 1, 1, 1),\n",
    "    \"kernel_size\": hp.quniform(\"kernel_size\",1, 2, 1),\n",
    "    \"pool_size\": hp.quniform(\"pool_size\",1, 3, 1),\n",
    "    \"pool_stride\": hp.quniform(\"pool_stride\", 2, 2, 1),\n",
    "    \"dense_layers\": hp.quniform(\"dense_layers\", 1, 5, 1),\n",
    "    \"dense_neurons\": hp.quniform(\"dense_neurons\", 1, 100, 1),\n",
    "    \"dropout_rate_dense\": hp.uniform(\"dropout_rate_dense\",0,1),\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', np.log(1e-02), np.log(1e-06)),\n",
    "    \"activation_fn\": hp.choice('activation_fn', [\"relu\"])\n",
    "}\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write best parameters as to disk\n",
    "with open('best_parameters.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in best.items():\n",
    "       writer.writerow([key, value])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file back in as dictionary \n",
    "with open('best_parameters.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    mydict = dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>run_time</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'activation_fn': 'relu', 'conv_filters': 23.0...</td>\n",
       "      <td>169.399304</td>\n",
       "      <td>0.650419</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.719617</td>\n",
       "      <td>0.679297</td>\n",
       "      <td>0.857303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'activation_fn': 'relu', 'conv_filters': 73.0...</td>\n",
       "      <td>207.985883</td>\n",
       "      <td>0.637404</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.801305</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.855211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params    run_time  val_loss  \\\n",
       "0  {'activation_fn': 'relu', 'conv_filters': 23.0...  169.399304  0.650419   \n",
       "1  {'activation_fn': 'relu', 'conv_filters': 73.0...  207.985883  0.637404   \n",
       "\n",
       "   val_accuracy  train_loss  train_accuracy  train_f1  \n",
       "0         0.725    0.719617        0.679297  0.857303  \n",
       "1         0.725    0.801305        0.725000  0.855211  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"bayes_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_fn': 0,\n",
       " 'conv_filters': 73.0,\n",
       " 'conv_layers': 6.0,\n",
       " 'conv_stride': 1.0,\n",
       " 'dense_layers': 3.0,\n",
       " 'dense_neurons': 9.0,\n",
       " 'dropout_rate_dense': 0.813556538245506,\n",
       " 'kernel_size': 1.0,\n",
       " 'learning_rate': 0.00033762014425420175,\n",
       " 'pool_size': 2.0,\n",
       " 'pool_stride': 2.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "* Write best to disk\n",
    "* Write individual steps to disk as csv\n",
    "* Implement a \"predict\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "NUM_MODELS = 10\n",
    "\n",
    "# build models\n",
    "# config_file_ensemble = placeholder\n",
    "models = []\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    models[i] = cnn_model(x_train_url, y_train_url, 2, config_v2, name=\"ensemble_model_v1_{}\".format())\n",
    "    models[i].train(on_tpu=True, epochs= 10, batch_size=256)\n",
    "    \n",
    "# prediction\n",
    "predictions = np.array()\n",
    "for i in range(len(models)):\n",
    "    np.append(predictions, models[i].predict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_v1 = {\n",
    "    \"name\": \"vanilla_v1\",\n",
    "    \"num_classes\": 10,\n",
    "    \"conv_neurons\": [32,64,32,16],\n",
    "    \"conv_dropout_layers\": [0.]*4,\n",
    "    \"dense_neurons\": [100,10],\n",
    "    \"dense_dropout_layers\": [0.]*2,\n",
    "    \"kernel_size\": (3,3),\n",
    "    \"pool_size\": (2,2),\n",
    "    \"activation_fn\": \"relu\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e04,\n",
    "    \"optimizer\": \"adam\", #tf.keras.optimizers.Adam,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Numpy data from GCS\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_train.npy', binary_mode=True))\n",
    "x_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_trainp.npy', binary_mode=True))\n",
    "y_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_test.npy', binary_mode=True))\n",
    "x_test_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_test.npy', binary_mode=True))\n",
    "y_test_gs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train_mnist = np.reshape(x_train_mnist, (-1,28,28,1))\n",
    "x_test_mnist = np.reshape(x_test_mnist, (-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time  # for mnist training \n",
    "\n",
    "config_v2 = {    \n",
    "    \"conv_layers\": 1,\n",
    "    \"conv_filters\": 16,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":4,\n",
    "    \"dense_layers\": 1,\n",
    "    \"dense_neurons\": 10,\n",
    "    \"dropout_rate_conv\": 0.2,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "# x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "# y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m2 = cnn_model(x_train_mnist, y_train_mnist, 10,config_v2)\n",
    "m2.train(on_tpu=True, epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading and reshaping MNIST data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['no_car', 'car']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to save files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def get_files(mypath):\n",
    "    \"\"\"\n",
    "    list all files in a folder\n",
    "    \n",
    "    args: \n",
    "        mypath: directory (str)\n",
    "    returns:\n",
    "        f: list of all file paths relative to directory specified\n",
    "    \"\"\"\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for file in filenames:\n",
    "            f.extend([os.path.join(dirpath,file)])\n",
    "    return f\n",
    "\n",
    "\n",
    "def copy_folder_files_to_gs(folder, gs_folder):\n",
    "    \"\"\"\n",
    "    Copy model.h5 over to Google Cloud Storage\n",
    "    \"\"\"\n",
    "    for file in get_files(folder):\n",
    "        #file_path = os.path.join(folder, \"/\".join(file.split(\"/\")[:]))\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "\n",
    "        with file_io.FileIO(file, mode='r') as input_f:\n",
    "            with file_io.FileIO(gs_folder, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read())\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger1 = \"./logs/1552722826.7797823/events.out.tfevents.1552722838.hsg-iwi-imr2\"\n",
    "logger11=\"TPU_first_test (15-03-19).ipynb\"\n",
    "bucket = \"gs://data-imr-unisg/a\"\n",
    "logger2 = \"logs\"\n",
    "with file_io.FileIO(logger1, mode='r') as input_f:\n",
    "            with file_io.FileIO(bucket, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read()\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
