{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab Link (TPU tutorial):\n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=SaYPv_aKId2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "We train our own network on a TPU. The network consists out of stacked conv, pool and dropout layers\n",
    "\n",
    "## Open To Do's:\n",
    "* Add a mathematical part, that checks whether or not the stride size of the pooling layer is too large in combination with the amount of layers and will cause an error\n",
    "* Larger Hyperparameter testing\n",
    "* Replace dropout by Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install keras-metrics\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from io import BytesIO \n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\t\n",
    "    \"\"\"Precision metric.\t\n",
    "    Only computes a batch-wise average of precision. Computes the precision, a\n",
    "    metric for multi-label classification of how many selected items are\n",
    "    relevant.\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\t\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\t\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\t\n",
    "    \"\"\"Recall metric.\t\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\t\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\t\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\t\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\t\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Computes the F1 Score\n",
    "    Only computes a batch-wise average of recall. Computes the recall, a metric\n",
    "    for multi-label classification of how many relevant items are selected.\t\n",
    "    \"\"\"\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return (2 * p * r) / (p + r + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model:\n",
    "    def __init__(self, x_train, y_train,num_classes, config, name=None):\n",
    "        \"\"\"\n",
    "        initializes the model and defines the graph. There will always be one more\n",
    "        dense layer than defined.\n",
    "        \"\"\"\n",
    "        # hard features\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss = \"binary_crossentropy\"\n",
    "        \n",
    "        # mutable features\n",
    "        self.conv_layers = int(config[\"conv_layers\"])\n",
    "        self.conv_filters = int(config[\"conv_filters\"])\n",
    "        self.conv_stride = (int(config[\"conv_stride\"]), int(config[\"conv_stride\"]))\n",
    "        self.kernel_size = (int(config[\"kernel_size\"]), int(config[\"kernel_size\"]))\n",
    "        self.pool_size = (int(config[\"pool_size\"]), int(config[\"pool_size\"]))\n",
    "        self.pool_stride = (int(config[\"pool_stride\"]),int(config[\"pool_stride\"]))\n",
    "        self.dense_layers = int(config[\"dense_layers\"])\n",
    "        self.dense_neurons = int(config[\"dense_neurons\"])\n",
    "        self.dropout_rate_dense = config[\"dropout_rate_dense\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.activation_fn = config[\"activation_fn\"]\n",
    "        # name describes characteristics \n",
    "        if bool(name):\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = \"conv_size_{}_filters_{}_kernel_{}_pool_{}_dense_{}_dropout_\\\n",
    "                dense_{}_lr_{}_act_{}\".format(self.conv_layers, self.conv_filters, self.kernel_size[0], \n",
    "                self.pool_size[0], self.dense_layers, self.dropout_rate_dense, \n",
    "                self.learning_rate, self.activation_fn)\n",
    "        \n",
    "        # check whether input is numpy format or a link to google cloud storage\n",
    "        if isinstance(x_train, str):\n",
    "            if \"gs\" in x_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(x_train, binary_mode=True))\n",
    "                self.x_train = np.load(f)\n",
    "        else:\n",
    "            self.x_train = x_train\n",
    "            \n",
    "        if isinstance(y_train, str):\n",
    "            if \"gs\" in y_train:\n",
    "                f = BytesIO(file_io.read_file_to_string(y_train, binary_mode=True))\n",
    "                self.y_train = np.load(f)\n",
    "        else:\n",
    "            self.y_train = y_train\n",
    "            \n",
    "        # create train and validation sets\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(self.x_train,\n",
    "                                                            self.y_train,\n",
    "                                                            train_size=0.8,\n",
    "                                                            random_state = 1)\n",
    "        input_shape = self.x_train.shape[1:]\n",
    "\n",
    "        # defining the model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=self.conv_filters,\n",
    "                         kernel_size=self.kernel_size,\n",
    "                         activation=self.activation_fn,\n",
    "                         input_shape=input_shape,\n",
    "                         padding=\"SAME\",\n",
    "                         strides=self.conv_stride\n",
    "                        ))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                 strides=self.pool_stride))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        for i in range(self.conv_layers-1):\n",
    "            model.add(Conv2D(filters=self.conv_filters,\n",
    "                             kernel_size=self.kernel_size,\n",
    "                             activation=self.activation_fn,\n",
    "                             padding=\"SAME\",\n",
    "                             strides=self.conv_stride\n",
    "                             #input_shape=input_shape\n",
    "                          ))\n",
    "            model.add(MaxPooling2D(pool_size=self.pool_size,\n",
    "                     strides=self.pool_stride))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        for i in range(self.dense_layers):\n",
    "            model.add(Dense(self.dense_neurons, activation=self.activation_fn))\n",
    "            model.add(Dropout(self.dropout_rate_dense))\n",
    "\n",
    "        model.add(Dense(num_classes, activation='softmax')) # softmax remains unchanged\n",
    "        self.model = model   \n",
    "        \n",
    "    def train(self, epochs, batch_size, learning_rate=None, optimizer=None, loss=None, verbose=False, on_tpu=False):\n",
    "        \"\"\"\n",
    "        trains the model.\n",
    "\n",
    "        If the initial config file contained parameters for training then\n",
    "        these dont have to be defined but can still be overridden\n",
    "        \"\"\" \n",
    "        if learning_rate is None:\n",
    "            learning_rate = self.learning_rate \n",
    "        if optimizer is None:\n",
    "            optimizer = self.optimizer \n",
    "        if loss is None:\n",
    "            loss = self.loss \n",
    "                  \n",
    "        date_time = datetime.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "        log_name = \"gs://data-imr-unisg/logs/{}_{}\".format(self.name, date_time)\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_name,\n",
    "                                write_graph=True,\n",
    "                                write_images=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                                patience=5)\n",
    "        \n",
    "        # model has to be compiled differently when on tpu\n",
    "        if on_tpu:\n",
    "            \n",
    "            tpu_model = tf.contrib.tpu.keras_to_tpu_model(self.model,\n",
    "                                                          strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(\"dominique-c-a-paul\")))\n",
    "            tpu_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['sparse_categorical_accuracy', f1_score])\n",
    "        \n",
    "            def train_gen(batch_size):\n",
    "                \"\"\"\n",
    "                generator function for training the model on a tpu\n",
    "                \"\"\"\n",
    "                while True:\n",
    "                    offset = np.random.randint(0, self.x_train.shape[0] - batch_size)\n",
    "                    # print(self.x_train[offset:offset+batch_size].shape, self.y_train[offset:offset + batch_size].shape)\n",
    "                    yield self.x_train[offset:offset+batch_size], self.y_train[offset:offset + batch_size]\n",
    "\n",
    "            # has to be optimised to really train a epoch with full data\n",
    "            self.hist = tpu_model.fit_generator(\n",
    "                train_gen(batch_size),\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=10,\n",
    "                validation_data=(self.x_val, self.y_val),\n",
    "                callbacks=[tensorboard_callback]\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            self.y_train = tf.keras.utils.to_categorical(self.y_train, 2 )\n",
    "            self.y_val = tf.keras.utils.to_categorical(self.y_val, 2 )\n",
    "            self.model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', f1])\n",
    "            self.model.fit(self.x_train, self.y_train, epochs=epochs, batch_size=batch_size, \n",
    "                           verbose=verbose, callbacks=[tensorboard_callback], validation_data=(self.x_val, self.y_val))\n",
    "\n",
    "        def predict(self, x_data):\n",
    "            cpu_model = tpu_model.sync_to_cpu()\n",
    "            predictions = cpu_model.predict(x_data)\n",
    "            return predictions\n",
    "\n",
    "        \n",
    "        def save_model(self, file_path):\n",
    "            raise notImplementedError()\n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 299, 299, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 149, 149, 128)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 149, 149, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 74, 74, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 74, 74, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 37, 37, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 20)                829460    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,279,566\n",
      "Trainable params: 1,278,542\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/1\n",
      "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_40'), TensorSpec(shape=(16, 299, 299, 3), dtype=tf.float32, name='conv2d_25_input_10'), TensorSpec(shape=(16, 1), dtype=tf.float32, name='dense_32_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_25_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 57.24187898635864 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ff2b486932eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_v1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_tpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7039db3c553e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, learning_rate, optimizer, loss, verbose, on_tpu)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfeed_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0mtpu_model_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutfeed_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m     ], infeed_dict)\n\u001b[0m\u001b[1;32m   1259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfeed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yes/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_v1 = {    \n",
    "    \"conv_layers\": 4,\n",
    "    \"conv_filters\": 128,\n",
    "    \"conv_stride\": 1,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":2,\n",
    "    \"pool_stride\": 2,\n",
    "    \"dense_layers\": 5,\n",
    "    \"dense_neurons\": 20,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m1 = cnn_model(x_train_url, y_train_url, 2, config_v1)\n",
    "m1.model.summary()\n",
    "m1.train(on_tpu=True, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m_opt=cnn_model(x_train_url, y_train_url, 2, config_v2)\n",
    "hist = m_opt.train(on_tpu=True, epochs=2, batch_size=128)\n",
    "val_loss = np.max(m_opt.hist.history[\"val_loss\"])\n",
    "val_accuracy = np.max(m_opt.hist.history[\"val_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_opt.hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !gcloud alpha compute tpus list\n",
    "# !gcloud compute tpus stop dominique-c-a-paul && gcloud compute tpus start dominique-c-a-paul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: \n",
    "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a\n",
    "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 299, 299, 109)     1417      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 150, 150, 109)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 150, 150, 109)     436       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 150, 150, 109)     47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 75, 75, 109)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 75, 75, 109)       436       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 75, 75, 109)       47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 38, 38, 109)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 38, 38, 109)       436       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 38, 38, 109)       47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 19, 19, 109)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 19, 19, 109)       436       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 19, 19, 109)       47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 10, 10, 109)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 10, 10, 109)       436       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 10, 10, 109)       47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 5, 5, 109)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 5, 5, 109)         436       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 5, 5, 109)         47633     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 3, 3, 109)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 3, 3, 109)         436       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 981)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 20)                19640     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 310,369                              \n",
      "Trainable params: 308,843                          \n",
      "Non-trainable params: 1,526                        \n",
      "_________________________________________________________________\n",
      "None                                               \n",
      "  0%|          | 0/2 [00:04<?, ?it/s, best loss: ?]INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/2                                          \n",
      "  0%|          | 0/2 [00:29<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_60'), TensorSpec(shape=(32, 299, 299, 3), dtype=tf.float32, name='conv2d_29_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_35_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_29_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 51.58795881271362 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 15:33 - loss: 1.7017 - sparse_categorical_accuracy: 0.6055 - f1_score: 0.7166\n",
      "                                                   \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 7:02 - loss: 1.6058 - sparse_categorical_accuracy: 0.6230 - f1_score: 0.7791 \n",
      "                                                  \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 4:11 - loss: 1.5139 - sparse_categorical_accuracy: 0.6328 - f1_score: 0.8118\n",
      "                                                  \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 2:44 - loss: 1.4384 - sparse_categorical_accuracy: 0.6377 - f1_score: 0.8257\n",
      "                                                  \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 1:51 - loss: 1.3729 - sparse_categorical_accuracy: 0.6328 - f1_score: 0.8289\n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 1:15 - loss: 1.2886 - sparse_categorical_accuracy: 0.6426 - f1_score: 0.8319\n",
      "                                                  \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 49s - loss: 1.2462 - sparse_categorical_accuracy: 0.6490 - f1_score: 0.8373 \n",
      "                                                   \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 29s - loss: 1.2044 - sparse_categorical_accuracy: 0.6582 - f1_score: 0.8394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 13s - loss: 1.1611 - sparse_categorical_accuracy: 0.6615 - f1_score: 0.8391\n",
      "  0%|          | 0/2 [02:28<?, ?it/s, best loss: ?]INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(30,), dtype=tf.int32, name='core_id_70'), TensorSpec(shape=(30, 299, 299, 3), dtype=tf.float32, name='conv2d_29_input_10'), TensorSpec(shape=(30, 1), dtype=tf.float32, name='dense_35_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_29_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 58.31293487548828 secs\n",
      "                                                   \n",
      "10/10 [==============================]             \n",
      " - 183s 18s/step - loss: 1.1348 - sparse_categorical_accuracy: 0.6664 - f1_score: 0.8419 - val_loss: 0.6958 - val_sparse_categorical_accuracy: 0.6375 - val_f1_score: 0.8406\n",
      "\n",
      "Epoch 2/2                                          \n",
      " 1/10 [==>...........................]             \n",
      " - ETA: 16s - loss: 0.8722 - sparse_categorical_accuracy: 0.6836 - f1_score: 0.8597\n",
      "                                                   \n",
      " 2/10 [=====>........................]             \n",
      " - ETA: 14s - loss: 0.8714 - sparse_categorical_accuracy: 0.7031 - f1_score: 0.8533\n",
      "                                                   \n",
      " 3/10 [========>.....................]             \n",
      " - ETA: 13s - loss: 0.8885 - sparse_categorical_accuracy: 0.7148 - f1_score: 0.8596\n",
      "                                                   \n",
      " 4/10 [===========>..................]             \n",
      " - ETA: 11s - loss: 0.8516 - sparse_categorical_accuracy: 0.7197 - f1_score: 0.8603\n",
      "                                                   \n",
      " 5/10 [==============>...............]             \n",
      " - ETA: 9s - loss: 0.8363 - sparse_categorical_accuracy: 0.7219 - f1_score: 0.8617 \n",
      "                                                  \n",
      " 6/10 [=================>............]             \n",
      " - ETA: 7s - loss: 0.8259 - sparse_categorical_accuracy: 0.7194 - f1_score: 0.8596\n",
      "                                                  \n",
      " 7/10 [====================>.........]             \n",
      " - ETA: 5s - loss: 0.8211 - sparse_categorical_accuracy: 0.7254 - f1_score: 0.8611\n",
      "                                                  \n",
      " 8/10 [=======================>......]             \n",
      " - ETA: 3s - loss: 0.8227 - sparse_categorical_accuracy: 0.7241 - f1_score: 0.8625\n",
      "                                                  \n",
      " 9/10 [==========================>...]             \n",
      " - ETA: 1s - loss: 0.8099 - sparse_categorical_accuracy: 0.7240 - f1_score: 0.8624\n",
      "                                                  \n",
      "10/10 [==============================]             \n",
      " - 20s 2s/step - loss: 0.8063 - sparse_categorical_accuracy: 0.7258 - f1_score: 0.8617 - val_loss: 0.6622 - val_sparse_categorical_accuracy: 0.7292 - val_f1_score: 0.8406\n",
      "\n",
      "_________________________________________________________________            \n",
      "Layer (type)                 Output Shape              Param #               \n",
      "=================================================================            \n",
      "conv2d_36 (Conv2D)           (None, 299, 299, 47)      188                   \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_36 (MaxPooling (None, 149, 149, 47)      0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_36 (Batc (None, 149, 149, 47)      188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_37 (Conv2D)           (None, 149, 149, 47)      2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_37 (MaxPooling (None, 74, 74, 47)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_37 (Batc (None, 74, 74, 47)        188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_38 (Conv2D)           (None, 74, 74, 47)        2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_38 (MaxPooling (None, 36, 36, 47)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_38 (Batc (None, 36, 36, 47)        188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_39 (Conv2D)           (None, 36, 36, 47)        2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_39 (MaxPooling (None, 17, 17, 47)        0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_39 (Batc (None, 17, 17, 47)        188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_40 (Conv2D)           (None, 17, 17, 47)        2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_40 (MaxPooling (None, 8, 8, 47)          0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_40 (Batc (None, 8, 8, 47)          188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_41 (Conv2D)           (None, 8, 8, 47)          2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_41 (MaxPooling (None, 3, 3, 47)          0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_41 (Batc (None, 3, 3, 47)          188                   \n",
      "_________________________________________________________________            \n",
      "conv2d_42 (Conv2D)           (None, 3, 3, 47)          2256                  \n",
      "_________________________________________________________________            \n",
      "max_pooling2d_42 (MaxPooling (None, 1, 1, 47)          0                     \n",
      "_________________________________________________________________            \n",
      "batch_normalization_42 (Batc (None, 1, 1, 47)          188                   \n",
      "_________________________________________________________________            \n",
      "flatten_7 (Flatten)          (None, 47)                0                     \n",
      "_________________________________________________________________            \n",
      "dense_36 (Dense)             (None, 72)                3456                  \n",
      "_________________________________________________________________            \n",
      "dropout_29 (Dropout)         (None, 72)                0                     \n",
      "_________________________________________________________________            \n",
      "dense_37 (Dense)             (None, 72)                5256                  \n",
      "_________________________________________________________________            \n",
      "dropout_30 (Dropout)         (None, 72)                0                     \n",
      "_________________________________________________________________            \n",
      "dense_38 (Dense)             (None, 72)                5256                  \n",
      "_________________________________________________________________            \n",
      "dropout_31 (Dropout)         (None, 72)                0                     \n",
      "_________________________________________________________________            \n",
      "dense_39 (Dense)             (None, 72)                5256                  \n",
      "_________________________________________________________________            \n",
      "dropout_32 (Dropout)         (None, 72)                0                     \n",
      "_________________________________________________________________            \n",
      "dense_40 (Dense)             (None, 2)                 146                   \n",
      "=================================================================            \n",
      "Total params: 34,410                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 33,752                                                     \n",
      "Non-trainable params: 658                                                    \n",
      "_________________________________________________________________            \n",
      "None                                                                         \n",
      " 50%|     | 1/2 [03:57<03:53, 233.19s/it, best loss: 0.6621878743171692]INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.2:8470) for TPU system metadata.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 14199058594203781822)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14049723852272536769)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 5429159450907858974)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13595952370891144489)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9344076617208949970)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 610572590359637876)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13651878177060175894)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5845260325198881210)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5528366968568110419)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4474525275461398485)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 6543809489826326675)\n",
      "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
      "Epoch 1/2                                                                    \n",
      " 50%|     | 1/2 [04:28<03:53, 233.19s/it, best loss: 0.6621878743171692]INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_80'), TensorSpec(shape=(32, 299, 299, 3), dtype=tf.float32, name='conv2d_36_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_40_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_36_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 54.57366633415222 secs\n",
      "INFO:tensorflow:Setting weights on TPU model.\n",
      " 1/10 [==>...........................]                                       \n",
      " - ETA: 16:24 - loss: 3.2896 - sparse_categorical_accuracy: 0.4727 - f1_score: 0.8494\n",
      "                                                                             \n",
      " 2/10 [=====>........................]                                       \n",
      " - ETA: 7:25 - loss: 3.2414 - sparse_categorical_accuracy: 0.4648 - f1_score: 0.8520 \n",
      "                                                                            \n",
      " 3/10 [========>.....................]                                       \n",
      " - ETA: 4:24 - loss: 3.2464 - sparse_categorical_accuracy: 0.4570 - f1_score: 0.8477\n",
      "                                                                            \n",
      " 4/10 [===========>..................]                                       \n",
      " - ETA: 2:53 - loss: 3.1590 - sparse_categorical_accuracy: 0.4512 - f1_score: 0.8507\n",
      "                                                                            \n",
      " 5/10 [==============>...............]                                       \n",
      " - ETA: 1:57 - loss: 3.1169 - sparse_categorical_accuracy: 0.4437 - f1_score: 0.8550\n",
      "                                                                            \n",
      " 6/10 [=================>............]                                       \n",
      " - ETA: 1:19 - loss: 3.0437 - sparse_categorical_accuracy: 0.4427 - f1_score: 0.8558\n",
      "                                                                            \n",
      " 7/10 [====================>.........]                                       \n",
      " - ETA: 52s - loss: 3.0030 - sparse_categorical_accuracy: 0.4453 - f1_score: 0.8571 \n",
      "                                                                             \n",
      " 8/10 [=======================>......]                                       \n",
      " - ETA: 31s - loss: 2.9624 - sparse_categorical_accuracy: 0.4453 - f1_score: 0.8571\n",
      "                                                                             \n",
      " 9/10 [==========================>...]                                       \n",
      " - ETA: 14s - loss: 2.8873 - sparse_categorical_accuracy: 0.4510 - f1_score: 0.8579\n",
      " 50%|     | 1/2 [06:34<03:53, 233.19s/it, best loss: 0.6621878743171692]INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(30,), dtype=tf.int32, name='core_id_90'), TensorSpec(shape=(30, 299, 299, 3), dtype=tf.float32, name='conv2d_36_input_10'), TensorSpec(shape=(30, 1), dtype=tf.float32, name='dense_40_target_10')]\n",
      "INFO:tensorflow:Overriding default placeholder.\n",
      "INFO:tensorflow:Remapping placeholder for conv2d_36_input\n",
      "INFO:tensorflow:Started compiling\n",
      "INFO:tensorflow:Finished compiling. Time elapsed: 60.19412636756897 secs\n",
      "                                                                             \n",
      "10/10 [==============================]                                       \n",
      " - 193s 19s/step - loss: 2.7872 - sparse_categorical_accuracy: 0.4594 - f1_score: 0.8581 - val_loss: 0.7130 - val_sparse_categorical_accuracy: 0.4042 - val_f1_score: 0.8406\n",
      "\n",
      "Epoch 2/2                                                                    \n",
      " 1/10 [==>...........................]                                       \n",
      " - ETA: 18s - loss: 2.0691 - sparse_categorical_accuracy: 0.5195 - f1_score: 0.8546\n",
      "                                                                             \n",
      " 2/10 [=====>........................]                                       \n",
      " - ETA: 16s - loss: 2.0474 - sparse_categorical_accuracy: 0.5195 - f1_score: 0.8546\n",
      "                                                                             \n",
      " 3/10 [========>.....................]                                       \n",
      " - ETA: 13s - loss: 1.8841 - sparse_categorical_accuracy: 0.5312 - f1_score: 0.8485\n",
      "                                                                             \n",
      " 4/10 [===========>..................]                                       \n",
      " - ETA: 11s - loss: 1.8258 - sparse_categorical_accuracy: 0.5400 - f1_score: 0.8481\n",
      "                                                                             \n",
      " 5/10 [==============>...............]                                       \n",
      " - ETA: 9s - loss: 1.7935 - sparse_categorical_accuracy: 0.5508 - f1_score: 0.8457 \n",
      "                                                                            \n",
      " 6/10 [=================>............]                                       \n",
      " - ETA: 7s - loss: 1.8299 - sparse_categorical_accuracy: 0.5436 - f1_score: 0.8481\n",
      "                                                                            \n",
      " 7/10 [====================>.........]                                       \n",
      " - ETA: 5s - loss: 1.8399 - sparse_categorical_accuracy: 0.5446 - f1_score: 0.8490\n",
      "                                                                            \n",
      " 8/10 [=======================>......]                                       \n",
      " - ETA: 3s - loss: 1.8230 - sparse_categorical_accuracy: 0.5420 - f1_score: 0.8500\n",
      "                                                                            \n",
      " 9/10 [==========================>...]                                       \n",
      " - ETA: 1s - loss: 1.8029 - sparse_categorical_accuracy: 0.5495 - f1_score: 0.8517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \n",
      "10/10 [==============================]                                       \n",
      " - 21s 2s/step - loss: 1.7958 - sparse_categorical_accuracy: 0.5496 - f1_score: 0.8535 - val_loss: 0.6401 - val_sparse_categorical_accuracy: 0.7250 - val_f1_score: 0.8406\n",
      "\n",
      "100%|| 2/2 [08:02<00:00, 238.13s/it, best loss: 0.6401130557060242]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "MAX_EVALS = 2\n",
    "\n",
    "\n",
    "# File to save first results\n",
    "out_file = 'bayes_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "# Write the headers to the file\n",
    "writer.writerow(['params','run_time', 'val_loss', 'val_accuracy', 'train_loss', 'train_accuracy', 'train_f1'])\n",
    "of_connection.close()\n",
    "\n",
    "# hyperparameter optimization with hyperopt\n",
    "def objective(params):\n",
    "    m_opt=cnn_model(x_train_url, y_train_url, 2, params)\n",
    "    print(m_opt.model.summary())\n",
    "    start = timer()\n",
    "    m_opt.train(on_tpu=True, epochs=2, batch_size=256)\n",
    "    run_time = timer() - start\n",
    "    val_loss = m_opt.hist.history[\"val_loss\"][-1]\n",
    "    val_accuracy = m_opt.hist.history[\"val_sparse_categorical_accuracy\"][-1]\n",
    "    val_f1 = m_opt.hist.history[\"val_f1_score\"][-1]\n",
    "    train_loss = m_opt.hist.history[\"loss\"][-1]\n",
    "    train_accuracy = m_opt.hist.history[\"sparse_categorical_accuracy\"][-1]\n",
    "    train_f1 = m_opt.hist.history[\"f1_score\"][-1]\n",
    "    \n",
    "    # adding lines to csv\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([params, run_time, val_loss, val_accuracy, train_loss, train_accuracy, train_f1])\n",
    "    of_connection.close()\n",
    "    \n",
    "    return {\"loss\": val_loss,\n",
    "            \"params\": params, \n",
    "            \"status\": hyperopt.STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    \"conv_layers\": hp.quniform(\"conv_layers\", 4, 8, 1),\n",
    "    \"conv_filters\": hp.quniform(\"conv_filters\", 2, 128, 1),\n",
    "    \"conv_stride\": hp.quniform(\"conv_stride\", 1, 1, 1),\n",
    "    \"kernel_size\": hp.quniform(\"kernel_size\",1, 2, 1),\n",
    "    \"pool_size\": hp.quniform(\"pool_size\",1, 3, 1),\n",
    "    \"pool_stride\": hp.quniform(\"pool_stride\", 2, 2, 1),\n",
    "    \"dense_layers\": hp.quniform(\"dense_layers\", 1, 5, 1),\n",
    "    \"dense_neurons\": hp.quniform(\"dense_neurons\", 1, 100, 1),\n",
    "    \"dropout_rate_dense\": hp.uniform(\"dropout_rate_dense\",0,1),\n",
    "    \"learning_rate\": hp.loguniform('learning_rate', np.log(1e-02), np.log(1e-06)),\n",
    "    \"activation_fn\": hp.choice('activation_fn', [\"relu\"])\n",
    "}\n",
    "\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "* Write best to disk\n",
    "* Write individual steps to disk as csv\n",
    "* Implement a \"predict\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters:\n",
    "NUM_MODELS = 10\n",
    "\n",
    "# build models\n",
    "# config_file_ensemble = placeholder\n",
    "models = []\n",
    "\n",
    "for i in range(NUM_MODELS):\n",
    "    models[i] = cnn_model(x_train_url, y_train_url, 2, config_v2, name=\"ensemble_model_v1_{}\".format())\n",
    "    models[i].train(on_tpu=True, epochs= 10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_v1 = {\n",
    "    \"name\": \"vanilla_v1\",\n",
    "    \"num_classes\": 10,\n",
    "    \"conv_neurons\": [32,64,32,16],\n",
    "    \"conv_dropout_layers\": [0.]*4,\n",
    "    \"dense_neurons\": [100,10],\n",
    "    \"dense_dropout_layers\": [0.]*2,\n",
    "    \"kernel_size\": (3,3),\n",
    "    \"pool_size\": (2,2),\n",
    "    \"activation_fn\": \"relu\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e04,\n",
    "    \"optimizer\": \"adam\", #tf.keras.optimizers.Adam,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Numpy data from GCS\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_train.npy', binary_mode=True))\n",
    "x_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_trainp.npy', binary_mode=True))\n",
    "y_train_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/x_test.npy', binary_mode=True))\n",
    "x_test_gs = np.load(f)\n",
    "f = BytesIO(file_io.read_file_to_string('gs://data-imr-unisg/np_array_files/class_labels_test.npy', binary_mode=True))\n",
    "y_test_gs = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train_mnist = np.reshape(x_train_mnist, (-1,28,28,1))\n",
    "x_test_mnist = np.reshape(x_test_mnist, (-1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time  # for mnist training \n",
    "\n",
    "config_v2 = {    \n",
    "    \"conv_layers\": 1,\n",
    "    \"conv_filters\": 16,\n",
    "    \"kernel_size\": 3,\n",
    "    \"pool_size\":4,\n",
    "    \"dense_layers\": 1,\n",
    "    \"dense_neurons\": 10,\n",
    "    \"dropout_rate_conv\": 0.2,\n",
    "    \"dropout_rate_dense\": 0.2,\n",
    "    \"learning_rate\": 1e-04,\n",
    "    \"activation_fn\": \"relu\"\n",
    "}\n",
    "\n",
    "# x_train_url = 'gs://data-imr-unisg/np_array_files/x_train.npy'\n",
    "# y_train_url = 'gs://data-imr-unisg/np_array_files/class_labels_trainp.npy'\n",
    "\n",
    "m2 = cnn_model(x_train_mnist, y_train_mnist, 10,config_v2)\n",
    "m2.train(on_tpu=True, epochs=40, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading and reshaping MNIST data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['no_car', 'car']\n",
    "\n",
    "\n",
    "cpu_model = tpu_model.sync_to_cpu()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_predictions(images, predictions):\n",
    "  n = images.shape[0]\n",
    "  nc = int(np.ceil(n / 4))\n",
    "  f, axes = pyplot.subplots(nc, 4)\n",
    "  for i in range(nc * 4):\n",
    "    y = i // 4\n",
    "    x = i % 4\n",
    "    axes[x, y].axis('off')\n",
    "    \n",
    "    label = LABEL_NAMES[np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i])\n",
    "    if i > n:\n",
    "      continue\n",
    "    axes[x, y].imshow(images[i])\n",
    "    axes[x, y].text(0.5, 0.5, label + '\\n%.3f' % confidence, fontsize=14)\n",
    "\n",
    "  pyplot.gcf().set_size_inches(8, 8)  \n",
    "\n",
    "plot_predictions(np.squeeze(x_test[:16]), \n",
    "                 tpu_model.predict(x_test[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to save files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def get_files(mypath):\n",
    "    \"\"\"\n",
    "    list all files in a folder\n",
    "    \n",
    "    args: \n",
    "        mypath: directory (str)\n",
    "    returns:\n",
    "        f: list of all file paths relative to directory specified\n",
    "    \"\"\"\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for file in filenames:\n",
    "            f.extend([os.path.join(dirpath,file)])\n",
    "    return f\n",
    "\n",
    "\n",
    "def copy_folder_files_to_gs(folder, gs_folder):\n",
    "    \"\"\"\n",
    "    Copy model.h5 over to Google Cloud Storage\n",
    "    \"\"\"\n",
    "    for file in get_files(folder):\n",
    "        #file_path = os.path.join(folder, \"/\".join(file.split(\"/\")[:]))\n",
    "        file_name = file.split(\"/\")[-1]\n",
    "\n",
    "        with file_io.FileIO(file, mode='r') as input_f:\n",
    "            with file_io.FileIO(gs_folder, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read())\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger1 = \"./logs/1552722826.7797823/events.out.tfevents.1552722838.hsg-iwi-imr2\"\n",
    "logger11=\"TPU_first_test (15-03-19).ipynb\"\n",
    "bucket = \"gs://data-imr-unisg/a\"\n",
    "logger2 = \"logs\"\n",
    "with file_io.FileIO(logger1, mode='r') as input_f:\n",
    "            with file_io.FileIO(bucket, mode='w+') as output_f:\n",
    "                output_f.write(input_f.read()\n",
    "                print(\"Saved {} to GCS\".format(logger1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
